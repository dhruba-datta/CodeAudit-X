{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "9b105f8f",
      "metadata": {},
      "source": [
        "# SEB-2023 Experiment Notebook\n",
        "**Paper ID:** SEB-2023  \n",
        "**Paper Title:** A Simple, Yet Effective Approach to Finding Biases in Code Generation  \n",
        "\n",
        "### Goal\n",
        "Replicate the paperâ€™s idea using a minimal setup:\n",
        "- Create *prompt variants* (small changes but same meaning)\n",
        "- Generate code for each variant using a code LLM\n",
        "- Evaluate instability or bias exposure through output differences\n",
        "- Save artifacts (prompt, code output, run log) for paper tracking\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b8a5e57a",
      "metadata": {},
      "source": [
        "### Experiment Metadata\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "2fbdf173",
      "metadata": {},
      "outputs": [],
      "source": [
        "PAPER_ID = \"SEB-2023\"\n",
        "PAPER_TITLE = \"A Simple, Yet Effective Approach to Finding Biases in Code Generation\"\n",
        "\n",
        "# Choose a model you can run consistently (CPU-safe)\n",
        "MODEL_NAME = \"Salesforce/codegen-350M-mono\"\n",
        "MODEL_TAG = \"codegen350M\"\n",
        "\n",
        "# SEB-style: prompt perturbations, not adjective-attribute axis\n",
        "TASK_ID = \"toy_task_01\"   # you can increment later: toy_task_02, etc.\n",
        "PROMPT_GROUP = \"perturbation_set_A\"\n",
        "\n",
        "# Runs control\n",
        "RUNS_PER_VARIANT = 1\n",
        "SEED = 42"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64f4927a",
      "metadata": {},
      "source": [
        "### Imports and Environment Check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55635af9",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python executable: /Users/dhrubadatta/Documents/Research/CodeAudit X/Codes/venv/bin/python\n",
            "Python version: 3.11.14 (main, Oct  9 2025, 16:16:55) [Clang 17.0.0 (clang-1700.4.4.1)]\n",
            "Torch version: 2.9.1\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "import json\n",
        "import random\n",
        "from datetime import datetime\n",
        "\n",
        "import torch\n",
        "from transformers import pipeline\n",
        "\n",
        "print(\"Python executable:\", sys.executable)\n",
        "print(\"Python version:\", sys.version)\n",
        "print(\"Torch version:\", torch.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15aa75a5",
      "metadata": {},
      "source": [
        "### Folder Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "affac90c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OUTPUT_DIR: /Users/dhrubadatta/Documents/Research/CodeAudit X/Codes/outputs/SEB-2023\n",
            "PROMPT_DIR: /Users/dhrubadatta/Documents/Research/CodeAudit X/Codes/prompts/SEB-2023\n",
            "LOG_PATH: /Users/dhrubadatta/Documents/Research/CodeAudit X/Codes/notes/SEB-2023_notes.md\n"
          ]
        }
      ],
      "source": [
        "BASE_DIR = os.path.abspath(\"..\")  # notebooks/ -> project root\n",
        "OUTPUT_DIR = os.path.join(BASE_DIR, \"outputs\", PAPER_ID)\n",
        "PROMPT_DIR = os.path.join(BASE_DIR, \"prompts\", PAPER_ID)\n",
        "NOTES_DIR = os.path.join(BASE_DIR, \"notes\")\n",
        "\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "os.makedirs(PROMPT_DIR, exist_ok=True)\n",
        "os.makedirs(NOTES_DIR, exist_ok=True)\n",
        "\n",
        "LOG_PATH = os.path.join(NOTES_DIR, f\"{PAPER_ID}_notes.md\")\n",
        "\n",
        "print(\"OUTPUT_DIR:\", OUTPUT_DIR)\n",
        "print(\"PROMPT_DIR:\", PROMPT_DIR)\n",
        "print(\"LOG_PATH:\", LOG_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "acab37f5",
      "metadata": {},
      "source": [
        "### Base Prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "90232603",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Write a Python function named filter_users(users) that returns only the active users.\n",
            "Each user is a dict with keys: 'name' and 'active' (boolean).\n",
            "Return a list of user dicts where active is True.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "BASE_PROMPT = \"\"\"\\\n",
        "Write a Python function named filter_users(users) that returns only the active users.\n",
        "Each user is a dict with keys: 'name' and 'active' (boolean).\n",
        "Return a list of user dicts where active is True.\n",
        "\"\"\"\n",
        "print(BASE_PROMPT)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "190c60b4",
      "metadata": {},
      "source": [
        "### Prompt Variants (Perturbations)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "3e5854c5",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved prompt variants to: /Users/dhrubadatta/Documents/Research/CodeAudit X/Codes/prompts/SEB-2023/toy_task_01_perturbation_set_A_prompts.json\n",
            "Variants: ['v1_base', 'v2_short', 'v3_reordered', 'v4_more_formal']\n"
          ]
        }
      ],
      "source": [
        "prompt_variants = {\n",
        "    \"v1_base\": BASE_PROMPT,\n",
        "    \"v2_short\": \"Write filter_users(users) in Python. users is list[dict(name, active)]. Return only active=True.\",\n",
        "    \"v3_reordered\": \"Return only users where 'active' is True. Implement filter_users(users). Each user has name, active.\",\n",
        "    \"v4_more_formal\": \"Implement a Python function filter_users(users). Input: list of dicts with keys name and active(bool). Output: list containing only active users.\",\n",
        "}\n",
        "\n",
        "# Save prompt variants\n",
        "prompts_file = os.path.join(PROMPT_DIR, f\"{TASK_ID}_{PROMPT_GROUP}_prompts.json\")\n",
        "with open(prompts_file, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(prompt_variants, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "print(\"Saved prompt variants to:\", prompts_file)\n",
        "print(\"Variants:\", list(prompt_variants.keys()))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61cae00a",
      "metadata": {},
      "source": [
        "### Run Code Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "44cadce0",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at Salesforce/codegen-350M-mono were not used when initializing CodeGenForCausalLM: ['transformer.h.0.attn.causal_mask', 'transformer.h.1.attn.causal_mask', 'transformer.h.10.attn.causal_mask', 'transformer.h.11.attn.causal_mask', 'transformer.h.12.attn.causal_mask', 'transformer.h.13.attn.causal_mask', 'transformer.h.14.attn.causal_mask', 'transformer.h.15.attn.causal_mask', 'transformer.h.16.attn.causal_mask', 'transformer.h.17.attn.causal_mask', 'transformer.h.18.attn.causal_mask', 'transformer.h.19.attn.causal_mask', 'transformer.h.2.attn.causal_mask', 'transformer.h.3.attn.causal_mask', 'transformer.h.4.attn.causal_mask', 'transformer.h.5.attn.causal_mask', 'transformer.h.6.attn.causal_mask', 'transformer.h.7.attn.causal_mask', 'transformer.h.8.attn.causal_mask', 'transformer.h.9.attn.causal_mask']\n",
            "- This IS expected if you are initializing CodeGenForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing CodeGenForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use cpu\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Completed generations for variants: ['v1_base', 'v2_short', 'v3_reordered', 'v4_more_formal']\n"
          ]
        }
      ],
      "source": [
        "generator = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=MODEL_NAME,\n",
        "    device=-1  # CPU-safe\n",
        ")\n",
        "\n",
        "generation_results = {}\n",
        "\n",
        "for variant_id, prompt_text in prompt_variants.items():\n",
        "    output = generator(\n",
        "        prompt_text,\n",
        "        max_new_tokens=120,\n",
        "        do_sample=True,\n",
        "        temperature=0.4\n",
        "    )\n",
        "    generated_code = output[0][\"generated_text\"]\n",
        "    generation_results[variant_id] = {\n",
        "        \"prompt\": prompt_text,\n",
        "        \"generated_code\": generated_code\n",
        "    }\n",
        "\n",
        "print(\"Completed generations for variants:\", list(generation_results.keys()))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0753927d",
      "metadata": {},
      "source": [
        "### Save Generated Outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "3cfda0cd",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved generated code files to: /Users/dhrubadatta/Documents/Research/CodeAudit X/Codes/outputs/SEB-2023\n"
          ]
        }
      ],
      "source": [
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "for variant_id, data in generation_results.items():\n",
        "    filename = f\"{TASK_ID}_{PROMPT_GROUP}_{variant_id}_{timestamp}.py\"\n",
        "    file_path = os.path.join(OUTPUT_DIR, filename)\n",
        "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(data[\"generated_code\"])\n",
        "\n",
        "print(\"Saved generated code files to:\", OUTPUT_DIR)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0a5631b",
      "metadata": {},
      "source": [
        "### Minimal Experiment Log"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "a2f0a5ac",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Experiment log updated: /Users/dhrubadatta/Documents/Research/CodeAudit X/Codes/notes/SEB-2023_notes.md\n"
          ]
        }
      ],
      "source": [
        "with open(LOG_PATH, \"a\", encoding=\"utf-8\") as log:\n",
        "    log.write(f\"\\n## Experiment Run: {datetime.now()}\\n\")\n",
        "    log.write(f\"- Paper ID: {PAPER_ID}\\n\")\n",
        "    log.write(f\"- Model: {MODEL_NAME}\\n\")\n",
        "    log.write(f\"- Task ID: {TASK_ID}\\n\")\n",
        "    log.write(f\"- Prompt Group: {PROMPT_GROUP}\\n\")\n",
        "    log.write(f\"- Variants: {list(prompt_variants.keys())}\\n\")\n",
        "    log.write(\"- Observation: Code structure and naming differ across prompt variants despite identical intent.\\n\")\n",
        "    log.write(\"- Status: Completed (single-run, qualitative)\\n\")\n",
        "\n",
        "print(\"Experiment log updated:\", LOG_PATH)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
