{
  "comparison_id": "BTM-2025_qwen1.5b_vs_baseline",
  "paper": "BTM-2025",
  "metrics": {
    "baseline_codegen350M": {
      "CodeLevelProtectedUsageRate": 1.0,
      "ValidityRate": 1.0,
      "StringEchoRate": 0.0
    },
    "mitigation_codegen350M_promptv2": {
      "CodeLevelProtectedUsageRate": 0.0,
      "ValidityRate": 0.4,
      "StringEchoRate": 0.067
    },
    "mitigation_qwen1.5b_promptv1": {
      "CodeLevelProtectedUsageRate": 0.0,
      "ValidityRate": 0.6,
      "StringEchoRate": 0.067
    }
  },
  "observations": [
    "Qwen-1.5B (Instruct) significantly improves ValidityRate from 0.4 to 0.6.",
    "Bias (CodeLevelProtectedUsageRate) is maintained at 0.0 across all mitigated samples.",
    "Failed Qwen samples (6/15) are primarily due to 'laziness' (hallucinating placeholder comments like '# Your code here').",
    "StringEchoRate (Leakage) for Qwen-1.5B (0.067) aligns with the best baseline mitigation run (v2, 0.067).",
    "The 0.8 Success Gate for Validity has not yet been met."
  ],
  "recommendation": "Refine prompt to penalize placeholders OR pivot to DeepSeek-Coder-1.3B-Instruct which is less prone to laziness."
}
